def _get_model_params(self, model):
        """Return model's parameters from state_dict in a dictionary format.
           Keys of return dict change depending on model type in self.args."""

        # Init dict
        params = dict()
        new_state_dict = OrderedDict()

        # Remove 'module' in state_dict's keys
        for k, v in model.state_dict().items():
            if 'module' in k:
                name = k[7:]
            else:
                name = k
            new_state_dict[name] = v

        # Create dict
        params['word_embs'] = new_state_dict['word_embs.weight'].cpu().numpy()
        params['ent_embs'] = new_state_dict['ent_embs.weight'].cpu().numpy()
        params['gram_embs'] = new_state_dict['gram_embs.weight'].cpu().numpy()
        params['W'] = new_state_dict['orig_linear.weight'].cpu().numpy().T  # Transpose here
        params['b'] = new_state_dict['orig_linear.bias'].cpu().numpy()

        if self.model_name in ['mention_prior', 'only_prior', 'only_prior_linear']:
            params['mention_embs'] = new_state_dict['mention_embs.weight'].cpu().numpy()
            params['ent_mention_embs'] = new_state_dict['ent_mention_embs.weight'].cpu().numpy()

        if self.model_name == 'only_prior_linear':
            logger.info("Extracting mention linear layer from model.")
            params['mention_linear_W'] = new_state_dict['mention_linear.weight'].cpu().numpy().T
            params['mention_linear_b'] = new_state_dict['mention_linear.bias'].cpu().numpy()

            if self.args.debug:
                print('MENTION LINEAR W')
                print(params['mention_linear_W'])
                print('\n\nMENTION LINEAR b')
                print(params['mention_linear_b'])

        if self.model_name == 'weigh_concat':
            params['weighing_linear'] = new_state_dict['weighing_linear.weight'].cpu().numpy().T  # Transpose here!

        return params

    def _get_ent_combined_mention_prior(self, params):

        gram_embs = params['gram_embs']
        ent_embs = params['ent_embs']
        ent_mention_embs = params['ent_mention_embs']

        if self.args.debug:
            print("ENT MENTION EMBS")
            print(ent_mention_embs[:5])

        ent_gram_embs = gram_embs[self.ent_gram_indices, :].mean(axis=1)

        if self.args.norm_gram:
            ent_gram_embs = normalize(ent_gram_embs)

        ent_combined_embs = np.concatenate((ent_embs, ent_gram_embs, ent_mention_embs), axis=1)

        return ent_combined_embs

    @staticmethod
    def _get_ent_combined_only_prior(params):

        ent_combined_embs = params['ent_mention_embs']

        return ent_combined_embs

    def _get_ent_combined_include_word(self, params):

        gram_embs = params['gram_embs']
        ent_embs = params['ent_embs']
        word_embs = params['word_embs']
        ent_mention_embs = params['ent_mention_embs']
        W = params['W']
        b = params['b']

        if self.args.debug:
            print("ENT MENTION EMBS")
            print(ent_mention_embs[:5])

        ent_gram_embs = gram_embs[self.ent_gram_indices, :].mean(axis=1)
        if self.args.norm_gram:
            ent_gram_embs = normalize(ent_gram_embs)

        ent_word_embs = word_embs[self.ent_word_indices, :].mean(axis=1)
        ent_word_embs = ent_word_embs @ W + b

        if self.args.norm_word:
            ent_word_embs = normalize(ent_word_embs)

        ent_combined_embs = np.concatenate((ent_embs, ent_gram_embs, ent_word_embs), axis=1)

        return ent_combined_embs

    def _get_ent_combined_include_gram(self, params):

        gram_embs = params['gram_embs']
        ent_embs = params['ent_embs']

        ent_gram_embs = gram_embs[self.ent_gram_indices, :].mean(axis=1)
        if self.args.norm_gram:
            ent_gram_embs = normalize(ent_gram_embs)

        ent_combined_embs = np.concatenate((ent_embs, ent_gram_embs), axis=1)

        return ent_combined_embs

    def _get_mention_combined_mention_prior(self, gram_indices, word_indices, context_indices, params):

        gram_embs = params['gram_embs']
        word_embs = params['word_embs']
        mention_embs = params['mention_embs']
        W = params['W']
        b = params['b']

        mention_gram_embs = gram_embs[gram_indices, :].mean(axis=1)
        if self.args.norm_gram:
            mention_gram_embs = normalize(mention_gram_embs)

        mention_word_embs = mention_embs[word_indices, :].mean(axis=1)
        if self.args.norm_mention:
            mention_word_embs = normalize(mention_word_embs)

        mention_context_embs = word_embs[context_indices, :].mean(axis=1)
        mention_context_embs = mention_context_embs @ W + b
        if self.args.norm_context:
            mention_context_embs = normalize(mention_context_embs)

        mention_combined_embs = np.concatenate((mention_context_embs, mention_gram_embs, mention_word_embs), axis=1)

        return mention_combined_embs

    def _get_mention_combined_only_prior(self, word_indices, params):

        mention_embs = params['mention_embs']

        mention_word_embs = mention_embs[word_indices, :].mean(axis=1)

        if self.args.norm_word:
            mention_word_embs = normalize(mention_word_embs)

        mention_combined_embs = mention_word_embs

        return mention_combined_embs

    def _get_mention_combined_only_prior_linear(self, word_indices, params):

        mention_embs = self._get_mention_combined_only_prior(word_indices, params)
        mention_combined_embs = mention_embs @ params['mention_linear_W'] + params['mention_linear_b']

        return mention_combined_embs

    def _get_mention_combined_include_word(self, gram_indices, word_indices, context_indices, params):

        gram_embs = params['gram_embs']
        word_embs = params['word_embs']
        W = params['W']
        b = params['b']

        mention_gram_embs = gram_embs[gram_indices, :].mean(axis=1)

        if self.args.norm_gram:
            mention_gram_embs = normalize(mention_gram_embs)

        mention_word_embs = word_embs[word_indices, :].mean(axis=1)
        mention_word_embs = mention_word_embs @ W + b

        if self.args.norm_word:
            mention_word_embs = normalize(mention_word_embs)

        mention_context_embs = word_embs[context_indices, :].mean(axis=1)
        mention_context_embs = mention_context_embs @ W + b

        if self.args.norm_context:
            mention_context_embs = normalize(mention_context_embs)

        mention_combined_embs = np.concatenate((mention_context_embs, mention_gram_embs, mention_word_embs), axis=1)

        return mention_combined_embs

    def _get_mention_combined_include_gram(self, gram_indices, context_indices, params):

        gram_embs = params['gram_embs']
        word_embs = params['word_embs']
        W = params['W']
        b = params['b']

        mention_gram_embs = gram_embs[gram_indices, :].mean(axis=1)
        if self.args.norm_gram:
            mention_gram_embs = normalize(mention_gram_embs)

        mention_context_embs = word_embs[context_indices, :].mean(axis=1)
        mention_context_embs = mention_context_embs @ W + b
        if self.args.norm_context:
            mention_context_embs = normalize(mention_context_embs)

        mention_combined_embs = np.concatenate((mention_context_embs, mention_gram_embs), axis=1)

        return mention_combined_embs

    def _get_mention_combined_embs(self, params, data):
        """get mention embeddings for different models."""

        if data == 'wiki':
            gram_indices = self.wiki_mention_gram_indices[self.wiki_mask, :]
            word_indices = self.wiki_mention_word_indices[self.wiki_mask, :]
            context_indices = self.wiki_mention_context_indices[self.wiki_mask, :]
        elif data == 'conll':
            gram_indices = self.conll_mention_gram_indices
            word_indices = self.conll_mention_word_indices
            context_indices = self.conll_mention_context_indices
        else:
            logger.error('Dataset {} not implemented, choose between wiki and conll'.format(data))
            sys.exit(1)

        if self.model_name == 'only_prior':
            mention_combined_embs = self._get_mention_combined_only_prior(word_indices, params)
        elif self.model_name == 'only_prior_linear':
            mention_combined_embs = self._get_mention_combined_only_prior_linear(word_indices, params)
        elif self.model_name == 'include_word':
            mention_combined_embs = self._get_mention_combined_include_word(gram_indices, word_indices, context_indices, params)
        elif self.model_name in ['include_gram', 'weigh_concat']:
            mention_combined_embs = self._get_mention_combined_include_gram(gram_indices, context_indices, params)
        elif self.model_name == 'mention_prior':
            mention_combined_embs = self._get_mention_combined_mention_prior(gram_indices, word_indices, context_indices, params)
        else:
            logger.error("model type {} not recognized".format(self.model_name))
            sys.exit(1)

        if self.args.norm_final:
            mention_combined_embs = normalize(mention_combined_embs)

        return mention_combined_embs

    def _get_ent_combined_embs(self, params):
        """get entity embeddings for different models."""

        if self.model_name in ['only_prior', 'only_prior_linear']:
            ent_combined_embs = self._get_ent_combined_only_prior(params)
        elif self.model_name == 'include_word':
            ent_combined_embs = self._get_ent_combined_include_word(params)
        elif self.model_name in ['include_gram', 'weigh_concat']:
            ent_combined_embs = self._get_ent_combined_include_gram(params)
        elif self.model_name == 'mention_prior':
            ent_combined_embs = self._get_ent_combined_mention_prior(params)
        else:
            logger.error("model type {} not recognized".format(self.model_name))
            sys.exit(1)

        if self.args.norm_final:
            ent_combined_embs = normalize(ent_combined_embs)

        return ent_combined_embs

           @staticmethod
    def concat_weighted(w, emb_1, emb_2): return np.concatenate((w * emb_1, (1 - w) * emb_2), axis=1)


def _numpify_data_conll(self):
        """Function for CONLL data. Creates list of numpy arrays containing gram and word token ids
           for each mention and word tokens for context in abstract. Also output gold entity labels."""

        # Init lists
        all_mention_gram_indices = []
        all_mention_word_indices = []
        all_context_word_indices = []
        all_small_context_indices = []
        all_gold = []

        window = self.args.context_window

        # train / dev / test split
        if self.args.conll_split == 'train':
            func = is_training_doc
        elif self.args.conll_split == 'dev':
            func = is_dev_doc
        elif self.args.conll_split == 'test':
            func = is_test_doc
        else:
            logger.error("Conll split {} not recognized, choose one of train, dev, test".format(self.args.conll_split))
            sys.exit(1)

        # For each doc
        for text, gold_ents, _, _, _ in iter_docs(join(self.args.data_path, 'Conll', 'AIDA-YAGO2-dataset.tsv'), func):

            # Context
            context_word_tokens = [token.text.lower() for token in self.word_tokenizer.tokenize(text)]
            context_word_indices = [self.word_dict.get(token, 0) for token in context_word_tokens]
            context_word_indices = equalize_len(context_word_indices, self.args.max_context_size)

            # Create dictionaries of token char span to token span
            tokens = self.word_tokenizer.tokenize(text.lower())
            start2idx = {}
            end2idx = {}
            for token_idx, token in enumerate(tokens):
                start, end = token.span
                start2idx[start] = token_idx
                end2idx[end] = token_idx

            # For each mention
            context_word_token_ids = [self.word_dict.get(token, 0) for token in context_word_tokens]
            for ent_str, (begin, end) in gold_ents:
                if ent_str in self.ent_dict:
                    mention = text[begin:end]
                    all_gold.append(self.ent_dict[ent_str])
                    all_context_word_indices.append(context_word_indices)

                    # Small Context
                    # Create dictionaries of token id to positions
                    if begin not in start2idx:
                        begin = min(start2idx, key=lambda x: abs(x - begin))
                    if end not in end2idx:
                        end = min(end2idx, key=lambda x: abs(x - end))
                    start_token_idx = start2idx[begin]
                    end_token_idx = end2idx[end]

                    small_context_tokens = np.zeros(2 * self.args.context_window, dtype=np.int64)
                    if start_token_idx > window:
                        small_context_tokens[:window] = context_word_token_ids[start_token_idx - window:start_token_idx]
                    else:
                        small_context_tokens[:start_token_idx] = context_word_token_ids[:start_token_idx]
                    end_token_idx += 1
                    if len(context_word_token_ids) - end_token_idx > window:
                        small_context_tokens[window:] = context_word_token_ids[end_token_idx:end_token_idx + window]
                    else:
                        small_context_tokens[window:window + len(context_word_token_ids) - end_token_idx] = context_word_token_ids[end_token_idx:]
                    all_small_context_indices.append(small_context_tokens)

                    # Mention Gram
                    mention_gram_tokens = [token for token in self.gram_tokenizer(mention)]
                    mention_gram_indices = [self.gram_dict.get(token, 0) for token in mention_gram_tokens]
                    mention_gram_indices = equalize_len(mention_gram_indices, self.args.max_gram_size)
                    all_mention_gram_indices.append(np.array(mention_gram_indices).astype(np.int64))

                    # Mention Word
                    mention_word_tokens = [token.text.lower() for token in self.word_tokenizer.tokenize(mention)]
                    mention_word_indices = [self.word_dict.get(token, 0) for token in mention_word_tokens]
                    mention_word_indices = equalize_len(mention_word_indices, self.args.max_word_size)
                    all_mention_word_indices.append(np.array(mention_word_indices).astype(np.int64))

        output = {'gold': np.array(all_gold).astype(np.int32),
                  'mention_gram': np.vstack(all_mention_gram_indices).astype(np.int32),
                  'mention_word': np.vstack(all_mention_word_indices).astype(np.int32),
                  'context': np.vstack(all_context_word_indices).astype(np.int32),
                  'small_context': np.vstack(all_small_context_indices).astype(np.int32)
                  }
        return output